{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3ab754",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70347d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463450d",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2061aa",
   "metadata": {},
   "source": [
    "Linear Regression is a linear machine learning model that assumes a linear relationship between the input variables (x) and the single output variable (y). It uses this information to predict the value of y based on the value of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84814161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.linear_model import LinearRegression #Linear regression package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b5aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for linear regression\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ef0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Linear Regression model\n",
    "\n",
    "# LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "\n",
    "# fit_intercept - calculate the y-intercept of the model\n",
    "# copy_X - Changes the value of x entered in the model\n",
    "# positive - forces the coefficients to be positive\n",
    "\n",
    "#Simple model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a122e49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  [[6.72400e-02 0.00000e+00 3.24000e+00 ... 1.69000e+01 3.75210e+02\n",
      "  7.34000e+00]\n",
      " [9.23230e+00 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.66150e+02\n",
      "  9.53000e+00]\n",
      " [1.14250e-01 0.00000e+00 1.38900e+01 ... 1.64000e+01 3.93740e+02\n",
      "  1.05000e+01]\n",
      " ...\n",
      " [1.68118e+01 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.96900e+02\n",
      "  3.08100e+01]\n",
      " [4.92980e-01 0.00000e+00 9.90000e+00 ... 1.84000e+01 3.96900e+02\n",
      "  4.54000e+00]\n",
      " [2.98500e-02 0.00000e+00 2.18000e+00 ... 1.87000e+01 3.94120e+02\n",
      "  5.21000e+00]]\n",
      "Expected output :  [22.6 50.  23.   8.3 21.2 19.9 20.6 18.7 16.1 18.6  8.8 17.2 14.9 10.5\n",
      " 50.  29.  23.  33.3 29.4 21.  23.8 19.1 20.4 29.1 19.3 23.1 19.6 19.4\n",
      " 38.7 18.7 14.6 20.  20.5 20.1 23.6 16.8  5.6 50.  14.5 13.3 23.9 20.\n",
      " 19.8 13.8 16.5 21.6 20.3 17.  11.8 27.5 15.6 23.1 24.3 42.8 15.6 21.7\n",
      " 17.1 17.2 15.  21.7 18.6 21.  33.1 31.5 20.1 29.8 15.2 15.  27.5 22.6\n",
      " 20.  21.4 23.5 31.2 23.7  7.4 48.3 24.4 22.6 18.3 23.3 17.1 27.9 44.8\n",
      " 50.  23.  21.4 10.2 23.3 23.2 18.9 13.4 21.9 24.8 11.9 24.3 13.8 24.7\n",
      " 14.1 18.7 28.1 19.8 26.7 21.7 22.  22.9 10.4 21.9 20.6 26.4 41.3 17.2\n",
      " 27.1 20.4 16.5 24.4  8.4 23.   9.7 50.  30.5 12.3 19.4 21.2 20.3 18.8\n",
      " 33.4 18.5 19.6 33.2 13.1  7.5 13.6 17.4  8.4 35.4 24.  13.4 26.2  7.2\n",
      " 13.1 24.5 37.2 25.  24.1 16.6 32.9 36.2 11.   7.2 22.8 28.7]\n",
      "Predicted output :  [24.9357079  23.75163164 29.32638296 11.97534566 21.37272478 19.19148525\n",
      " 20.5717479  21.21154015 19.04572003 20.35463238  5.44119126 16.93688709\n",
      " 17.15482272  5.3928209  40.20270696 32.31327348 22.46213268 36.50124666\n",
      " 31.03737014 23.17124551 24.74815321 24.49939403 20.6595791  30.4547583\n",
      " 22.32487164 10.18932894 17.44286422 18.26103077 35.63299326 20.81960303\n",
      " 18.27218007 17.72047628 19.33772473 23.62254823 28.97766856 19.45036239\n",
      " 11.13170639 24.81843595 18.05294835 15.59712226 26.21043403 20.81140432\n",
      " 22.17349382 15.48367365 22.62261604 24.88561528 19.74754478 23.0465628\n",
      "  9.84579105 24.36378793 21.47849008 17.62118176 24.39160873 29.95102691\n",
      " 13.57219422 21.53645439 20.53306273 15.03433182 14.3232289  22.11929299\n",
      " 17.07321915 21.54141094 32.96766968 31.371599   17.7860591  32.75069556\n",
      " 18.74795323 19.21428022 19.41970047 23.08087809 22.87732816 24.06399098\n",
      " 30.52824406 28.71453508 25.90763165  5.17596718 36.8709072  23.76983849\n",
      " 27.26064379 19.25849042 28.41860517 19.3008798  18.94922353 38.00154059\n",
      " 39.44096748 23.72297885 24.83722534 16.52015743 25.9970546  16.73997072\n",
      " 15.48656983 13.52825536 24.12884363 30.76919578 22.18731163 19.8848644\n",
      "  0.42275479 24.86785849 16.05692    17.42486412 25.49798527 22.35171315\n",
      " 32.66562689 22.04428746 27.29799885 23.20302026  6.86196574 14.869251\n",
      " 22.31804948 29.18125768 33.22568234 13.24392523 19.67195771 20.7502616\n",
      " 12.02271319 23.50067006  5.55662571 19.87634689  9.27059783 44.81787339\n",
      " 30.56017983 12.44394048 17.33192202 21.48313292 23.52664913 20.49877266\n",
      " 35.09161099 13.22639935 20.70321163 35.35582833 19.45050576 13.81603561\n",
      " 14.15654562 23.03678503 15.07521258 30.9662041  25.23236632 15.43763716\n",
      " 24.06406534  9.93080346 15.01618901 21.06098873 32.87115732 27.80927747\n",
      " 25.91293794 15.27877362 30.97489404 27.81107682 14.5068157   7.57369946\n",
      " 28.3348068  25.04341153]\n"
     ]
    }
   ],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the boston housing dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd211c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 27.195965766883205\n",
      "Root Mean Squared Error: 5.214975145375403\n",
      "R^2 Score: 0.6733825506400195\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy\n",
    "# Accuracy of a linear model can't be calculated, the accuracy is predicted using error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Mean Squared Error\n",
    "print(\"Mean Squared Error:\",mean_squared_error(y_test, y_pred)) # tells you how close a regression line is to a set of points\n",
    "# squaring is necessary to remove any negative signs\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"Root Mean Squared Error:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# R2 Score\n",
    "print(\"R^2 Score:\", r2_score(y_test,y_pred)) # correlation between actual and predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d53dfc",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc3168",
   "metadata": {},
   "source": [
    "Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more independent variables. It produced output as 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.linear_model import LogisticRegression #Logistic regression package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfe7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for logistic regression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer[\"data\"]\n",
    "y = cancer['target']\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f46c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Logistic Regression model\n",
    "\n",
    "# LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "# intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, \n",
    "# multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# Simple model\n",
    "logreg = LogisticRegression(max_iter=3000) \n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818db063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  [[1.018e+01 1.753e+01 6.512e+01 ... 5.575e-02 3.055e-01 8.797e-02]\n",
      " [1.301e+01 2.222e+01 8.201e+01 ... 9.259e-03 2.295e-01 5.843e-02]\n",
      " [1.378e+01 1.579e+01 8.837e+01 ... 3.312e-02 1.859e-01 6.810e-02]\n",
      " ...\n",
      " [1.320e+01 1.743e+01 8.413e+01 ... 4.970e-02 2.767e-01 7.198e-02]\n",
      " [1.231e+01 1.652e+01 7.919e+01 ... 8.660e-02 2.618e-01 7.609e-02]\n",
      " [2.116e+01 2.304e+01 1.372e+02 ... 2.009e-01 2.822e-01 7.526e-02]]\n",
      "Expected output :  [1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
      " 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1\n",
      " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
      " 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0]\n",
      "Predicted output :  [1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
      " 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1\n",
      " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the boston housing dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b5976ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score: 0.9707602339181286\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logreg\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)) \u001b[38;5;66;03m#Predicted correctly/Total values\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Precision\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mprecision_score\u001b[49m(y_test, y_pred)) \u001b[38;5;66;03m#Proportion of positive identifications actually correct\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Recall\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall:\u001b[39m\u001b[38;5;124m\"\u001b[39m,recall_score(y_test, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculating the Accuracy\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", logreg.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred)) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred)) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4e4a6",
   "metadata": {},
   "source": [
    "## Decision Tree 1 - Iterative Dichotomiser 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925d85b",
   "metadata": {},
   "source": [
    "A decision tree is a structure that contains nodes (rectangular boxes) and edges(arrows) and is built from a dataset (with rows and columns). Each node is used to make a decision (decision node) or represent an outcome (leaf node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Required packages\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree Classifier package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Decision Tree 1 - Iterative Dichotomiser 3\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "#Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Decision Tree 1 - Iterative Dichotomiser 3 model\n",
    "\n",
    "# DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "# max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "#Simple model\n",
    "dtc = DecisionTreeClassifier(criterion='entropy',max_depth=3)\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the boston housing dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", dtc.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b95083",
   "metadata": {},
   "source": [
    "## Decision Tree - CART (Classification and Regression Tree Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772e16b",
   "metadata": {},
   "source": [
    "A classification algorithm for building a decision tree based on Gini's impurity index as splitting criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fd460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Decision Tree 2 - CART\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "#Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Decision Tree 1 - Iterative Dichotomiser 3 model\n",
    "\n",
    "# DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "# max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "#Simple model\n",
    "dtc_gini = DecisionTreeClassifier(criterion='gini',max_depth=3)\n",
    "dtc_gini.fit(X_train, y_train)\n",
    "y_pred = dtc_gini.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the boston housing dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", dtc.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bda0a",
   "metadata": {},
   "source": [
    "## Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaa775",
   "metadata": {},
   "source": [
    "### Type 1 - Gaussian Naive Bayes Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3d921",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes Algorithm is used when the features have continuous values, when the features are following a gaussian distribution i.e, normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussian Baive Bayes Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Naive Bayes Algorithm\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba991857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syntax for creating a Naive Bayes Algorithm\n",
    "\n",
    "#GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
    "\n",
    "#Link for explanation of the model: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    "#Simple model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the boston housing dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", gnb.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad89d0",
   "metadata": {},
   "source": [
    "## KMeans Clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d767f",
   "metadata": {},
   "source": [
    "The K-means clustering algorithm computes centroids and repeats until the optimal centroid is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your data as a csv file and load it as a data frame \n",
    "df = pd.read_csv('penguins.csv').dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d37ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find out the optimal number of KMeans clusters\n",
    "X = np.array(df.loc[:,['bill_length_mm','bill_depth_mm']]).reshape(-1, 2)\n",
    "\n",
    "# Determine optimal cluster number with elbow method\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    model = KMeans(n_clusters = i,init = 'k-means++',max_iter = 300,n_init = 10,random_state = 0)\n",
    "    model.fit(X)                              \n",
    "    wcss.append(model.inertia_)\n",
    "    \n",
    "# Show Elbow plot\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')                              \n",
    "plt.xlabel('Number of clusters')                      \n",
    "plt.ylabel('Within Cluster Sum of Squares (WCSS)')      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b147877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a KMeans Clustering Algorithm\n",
    "\n",
    "# KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=None, \n",
    "# copy_x=True, algorithm='lloyd')\n",
    "\n",
    "# n_clusters - amount of clusters\n",
    "# max_iter - maximum number of iteration\n",
    "# n_init - How often algorithm will run with different centroid\n",
    "# random_state - Choose random state for reproducibility\n",
    "\n",
    "# Link for explanation of the model: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "# Simple model\n",
    "kmeans = KMeans(n_clusters = 3,max_iter = 300,n_init = 10,random_state = 0)    \n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8046b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the boston housing dataset\n",
    "print(\"Input : \", X)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Class of output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d806a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the KMeans model using silhouette samples\n",
    "from sklearn.metrics import silhouette_samples\n",
    "for i, k in enumerate([2, 3, 4]):\n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    km = KMeans(n_clusters=k)\n",
    "    labels = km.fit_predict(X_train)\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(X_train, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',fontsize=16, fontweight='semibold', y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1715728",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours Classifier (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b7c7e",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbours Package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656edc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Naive Bayes Algorithm\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb84663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a KNN Classifier model\n",
    "\n",
    "# KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "# metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "# Link for explanation : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "# Simple model\n",
    "from sklearn.metrics import accuracy_score\n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(i)\n",
    "    print(\"accuracy= \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a KNN model based on the number of neighbors with the highest accuracy\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"accuracy= \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the wine dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf584ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", knn.score(X_test, y_test)) # Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) # Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235e35a",
   "metadata": {},
   "source": [
    "## Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aae5d1",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are powerful yet flexible supervised machine learning methods used for classification, regression, and, outliers' detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea7209",
   "metadata": {},
   "source": [
    "SVC fits the data provided, returning a \"best fit\" hyperplane that divides, or categorizes the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Support Vector Classifier Algorithm\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Support Vector Classifier model:\n",
    "\n",
    "\n",
    "# SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "# cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False,\n",
    "# random_state=None)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# A simple model:\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the breast cancer dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32997078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", svc.score(X_test, y_test)) # Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) # Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528985ac",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10981ddb",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is an unsupervised machine learning algorithm which can be used as a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space.\n",
    "\n",
    "PCA can't be used for prediction itself but can be used to reduce the dimensions in the training data to fit in for some algorithms such as KMeans etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Principal Component Analysis\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before PCA transformation\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db680f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After PCA transformation\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e19d4",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e61f7",
   "metadata": {},
   "source": [
    "Random Forest Classifier is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest Classifier package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13332d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a dataset for Random Forest Classifier Algorithm\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b564b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Random Forest Classifier model:\n",
    "\n",
    "\n",
    "# RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, \n",
    "# min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, \n",
    "# warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "# A simple model:\n",
    "rfm = RandomForestClassifier()\n",
    "rfm.fit(X_train, y_train)\n",
    "y_pred = rfm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the breast cancer dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", rfm.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6607af6",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1a0a0",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier Algorithm gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa003985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required packages\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting Classifier package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81147e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a dataset for Gradient Boosting Classifier Algorithm\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "#Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01becd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Syntax for creating a Gradient Boosting Classifier model:\n",
    "\n",
    "\n",
    "# GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, \n",
    "# criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "# min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None,\n",
    "# warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "# A simple model:\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10658900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the diabetes dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", gbc.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f703a",
   "metadata": {},
   "source": [
    "## Ada Boost Classifier Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec49c73",
   "metadata": {},
   "source": [
    "AdaBoost Classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.ensemble import AdaBoostClassifier # Ada Boost Classifier package\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset using make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,n_informative=2, n_redundant=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ee610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Ada Boost Classifier model:\n",
    "\n",
    "\n",
    "# AdaBoostClassifier(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', \n",
    "# random_state=None)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "# A simple model:\n",
    "abc = AdaBoostClassifier(n_estimators=100)\n",
    "abc.fit(X, y)\n",
    "y_pred = abc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc95855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the diabetes dataset\n",
    "print(\"Input : \", X)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", abc.score(X, y)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b641e49",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94596501",
   "metadata": {},
   "source": [
    "Gradient Boosting Regressor builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. \n",
    "In each stage a regression tree is fit on the negative gradient of the given loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Gradient Boosting Regressor package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset using make_regression \n",
    "X, y = make_regression(n_samples=200, n_features=20, n_informative=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d243f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Gradient Boosting Regressor model:\n",
    "\n",
    "\n",
    "# GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, \n",
    "# criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "# min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, \n",
    "# max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001,\n",
    "# ccp_alpha=0.0)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "# A simple model:\n",
    "gbr = GradientBoostingRegressor(n_estimators=200)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the diabetes dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e937b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Accuracy of a linear model can't be calculated, the accuracy is predicted using error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Mean Squared Error\n",
    "print(\"Mean Squared Error:\",mean_squared_error(y_test, y_pred)) # tells you how close a regression line is to a set of points\n",
    "# squaring is necessary to remove any negative signs\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"Root Mean Squared Error:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# R2 Score\n",
    "print(\"R^2 Score:\", r2_score(y_test,y_pred)) # correlation between actual and predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94099634",
   "metadata": {},
   "source": [
    "## Ada Boost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7a8dd",
   "metadata": {},
   "source": [
    "Ada Boost Regressor begins the process of regression by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e183e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor # Ada Boost Regressor package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset using make_regression\n",
    "X, y = make_regression(n_samples=200, n_features=20, n_informative=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for creating a Ada Boost Regressor:\n",
    "\n",
    "\n",
    "# AdaBoostRegressor(base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "\n",
    "# Link for explanation of terms : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "\n",
    "# A simple model:\n",
    "abr = AdaBoostRegressor(n_estimators=100)\n",
    "abr.fit(X_train, y_train)\n",
    "y_pred = abr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b64d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the diabetes dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Accuracy of a linear model can't be calculated, the accuracy is predicted using error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Mean Squared Error\n",
    "print(\"Mean Squared Error:\",mean_squared_error(y_test, y_pred)) # tells you how close a regression line is to a set of points\n",
    "# squaring is necessary to remove any negative signs\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"Root Mean Squared Error:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# R2 Score\n",
    "print(\"R^2 Score:\", r2_score(y_test,y_pred)) # correlation between actual and predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e427f15",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1f057",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Algorithm fits linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. It works very well with large scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent Classifier package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ceaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset using make_classification\n",
    "X, y = make_classification(n_samples=200, n_features=20, n_informative=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "sgdc.fit(X_train, y_train)\n",
    "y_pred = sgdc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the input & output\n",
    "\n",
    "# Input: A two-dimensional array which contains all the features of the diabetes dataset\n",
    "print(\"Input : \", X_test)\n",
    "\n",
    "# Output: The ouput array is the prediction\n",
    "print(\"Expected output : \", y_test)\n",
    "print(\"Predicted output : \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Accuracy\n",
    "# Importing necessary packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Accuracy\n",
    "print(\"Test Accuracy Score:\", sgdc.score(X_test, y_test)) #Predicted correctly/Total values\n",
    "\n",
    "# Precision\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average='weighted')) #Proportion of positive identifications actually correct\n",
    "\n",
    "# Recall\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average='weighted')) # Proportion of actual positives was identified correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fcc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
